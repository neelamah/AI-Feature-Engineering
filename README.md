# AI-Feature-Engineering
Feature Engineering means creating, modifying, or selecting variables (features) so that a machine learning model performs better.


Video Link : https://youtu.be/1Yw9sC0PNwY




Feature Engineering means creating, modifying, or selecting variables (features) so that a machine learning model performs better.

Feature Engineering = Transform + Scale + Create + Select + Clean features


Type:-
1ï¸âƒ£ Feature Transformation
    ğŸ”¹ Mathematical Transformations:- 
    Log transformation â†’ log(x)
    Square / square root â†’ xÂ², âˆšx
    Power transformation
    Box-Cox / Yeo-Johnson
    ğŸ‘‰ Used when data is skewed.

    ğŸ”¹ Encoding Categorical Variables
        One-Hot Encoding -> used for nominal caytegorial data(like gender, female which can not to rank)
        Label Encoding-> used for output column ( which is categorial)
        Ordinal Encoding -> used for caytegorial data (like poor, good exellent which can be ranked.)

2ï¸âƒ£ Feature Scaling:- 
    Used especially in:- (used in where we measure distance)
    Logistic Regression
    KNN
    SVM
    Neural Networks

    ğŸ”¹ Standardization
    (xâˆ’mean)/std    
    ğŸ”¹ Min-Max Scaling
    (xâˆ’min)/(maxâˆ’min)
    ğŸ”¹ Robust Scaling
      Uses median & IQR (good for outliers)  

3ï¸âƒ£ Feature Creation (Feature Construction)
    Creating new features from existing ones.
    Examples:
    BMI = weight / heightÂ²
    Age from Date of Birth
    Extract day/month/year from date
    Total purchase = quantity Ã— price
    Interaction features â†’ x1 * x2

4ï¸âƒ£ Feature Selection
Choosing important features.
ğŸ”¹ Filter Methods
    Correlation
    Chi-square
    ANOVA

5ï¸âƒ£ Handling Missing Values
    Mean/Median imputation
    Mode imputation
    KNN imputation
    Model-based imputation

6ï¸âƒ£ Handling Outliers
    Capping (Winsorization)
    IQR method
    Z-score method


ğŸ”Ÿ Dimensionality Reduction
    PCA
    t-SNE
    UMAP



Important concepts:-
ColumnTransformer 
pipeline
multicolarity